{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2f4e499",
   "metadata": {},
   "source": [
    "### Instructions \n",
    "- Download all the necessary libraries\n",
    "- Download ROM from https://github.com/openai/atari-py#roms\n",
    "- After run `python -m atari_py.import_roms <path to folder>`(I recommend the same that you're using)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71509698",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np \n",
    "import datetime\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05807696",
   "metadata": {},
   "source": [
    "### Define the enviroment and see the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089ce46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "observation = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "observation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb73360",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    observation, reward, done, info = env.step(0)# 0 means stay the same place(or do nothing)  \n",
    "plt.imshow(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b9118",
   "metadata": {},
   "source": [
    "#### Basic explanation\n",
    "- In the above image, we control the right side player\n",
    "- If the ball passes our paddle and ends up on the right, we receive a -1 penalty for losing, and if the ball crosses the opponent and ends up on the left, we receive a +1 penalty. The game ends when one of the players achieves 21 points.\n",
    "\n",
    "#### System definition:\n",
    "- State is the screen of game.\n",
    "- Action is going, down and stay\n",
    "\n",
    "#### Possible approaches\n",
    "- Because we're going to utilize a neural network as a policy, we'll need to provide enough data for it to figure out where the ball is going.\n",
    "- The state can be defined as the game's latest 10 frames to convey this information. After that, feed it to a NN. Alternatively, we might feed the frames to an RNN one by one so that it learns the game's sequence.\n",
    "- We utilize another technique for simplicity: we just remove two consecutive frames. Then use the resultant image as input to the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba942ae",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4984d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(new_frame:int, last_frame:int) -> int:\n",
    "    # inputs are 2 numpy 2d arrays\n",
    "    n_frame = new_frame.astype(np.int32)\n",
    "    # remove backgound colors\n",
    "    n_frame[(n_frame==144)|(n_frame==109)]=0 \n",
    "    l_frame = last_frame.astype(np.int32)\n",
    "    # remove backgound colors\n",
    "    l_frame[(l_frame==144)|(l_frame==109)]=0 \n",
    "    diff = n_frame - l_frame\n",
    "    # crop top and bot \n",
    "    diff = diff[35:195]\n",
    "    # down sample \n",
    "    diff=diff[::2,::2]\n",
    "    # convert to grayscale\n",
    "    diff = diff[:,:,0] * 299. / 1000 + diff[:,:,1] * 587. / 1000 + diff[:,:,2] * 114. / 1000\n",
    "    # rescale numbers between 0 and 1\n",
    "    max_val =diff.max() if diff.max()> abs(diff.min()) else abs(diff.min())\n",
    "    if max_val != 0:\n",
    "        diff = diff/max_val\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe6d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation, reward, done, info = env.step(2)\n",
    "plt.imshow(preprocess_frames(new_observation,observation),plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bacca6d",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "- We attempted to remove any superfluous information, such as color.\n",
    "- Because it did not give information, we cropped the bottom and top of the game screen.\n",
    "- The backgounds are removed\n",
    "\n",
    "Our current state is an 80*80 picture created by subtracting two consecutive frames, with most values being 0 but non-zero values where the paddles or ball have moved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4403c5f",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c1f41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(80, 80))\n",
    "inputs_reshape = tf.keras.layers.Reshape((80, 80, 1))(inputs)\n",
    "conv2d1 = tf.keras.layers.Conv2D(filters=10, kernel_size=20, padding='valid', \n",
    "                                 activation='relu', strides=(4,4), use_bias=False)(inputs_reshape)\n",
    "conv2d2 = tf.keras.layers.Conv2D(filters=20, kernel_size=10, padding='valid',\n",
    "                                 activation='relu', strides=(2,2), use_bias=False)(conv2d1)\n",
    "conv2d3 = tf.keras.layers.Conv2D(filters=40, kernel_size=3, padding='valid',\n",
    "                                 activation='relu', use_bias=False)(conv2d2)\n",
    "flattened_layer = tf.keras.layers.Flatten()(conv2d3)\n",
    "sigmoid_output = tf.keras.layers.Dense(1, activation='sigmoid',use_bias=False)(flattened_layer)\n",
    "model = tf.keras.models.Model(inputs=inputs,outputs=sigmoid_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1a4e0",
   "metadata": {},
   "source": [
    "### Defining Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715e7712",
   "metadata": {},
   "source": [
    "We can define the loss as:\n",
    "\n",
    "L = G ∇<sub>θ</sub> (−log(π))\n",
    "\n",
    "Where G is the reward from the state we are updating, π is the probablity of taking the action that we took when playing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = tf.keras.layers.Input(shape=(1,),name='episode_reward')\n",
    "def m_loss(episode_reward):\n",
    "    def loss(y_true,y_pred):\n",
    "        # feed in y_true as actual action taken \n",
    "        # if actual action was up, we feed 1 as y_true and otherwise 0\n",
    "        # y_pred is the network output(probablity of taking up action)        \n",
    "        tmp_pred = tf.keras.layers.Lambda(lambda x: tf.keras.backend.clip(x,0.05,0.95))(y_pred)\n",
    "        # we calculate log of probablity. y_pred is the probablity of taking up action\n",
    "        # y_true is 1 when we actually chose up, and 0 when we chose down\n",
    "        tmp_loss = tf.keras.layers.Lambda(lambda x:-y_true*tf.keras.backend.log(x)-\n",
    "                                          (1-y_true)*(tf.keras.backend.log(1-x)))(tmp_pred)\n",
    "        # multiply log of policy by reward\n",
    "        policy_loss = tf.keras.layers.Multiply()([tmp_loss,episode_reward])\n",
    "        return policy_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748b78a",
   "metadata": {},
   "source": [
    "### Creating optimizer and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = tf.keras.layers.Input(shape=(1,),name='episode_reward')\n",
    "policy_network_train = tf.keras.models.Model(inputs=[inputs,episode_reward],outputs=sigmoid_output)\n",
    "my_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "policy_network_train.compile(optimizer=my_optimizer,loss=m_loss(episode_reward),)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42326d",
   "metadata": {},
   "source": [
    "It's important to remember that policy network train and policy network model both employ the same layers (from inputs to outputs), and their weights and parameters are the same. As a result, we just utilize policy network for training and then policy network model for playing and simulating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0ff276",
   "metadata": {},
   "source": [
    "### Reward (maybe the most important step in RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b955e9",
   "metadata": {},
   "source": [
    "#### Problem definition\n",
    "- When the ball goes through our paddle (we score -1) or our opponent's\n",
    "paddle (we score +1), it appears like the environment just rewards us.\n",
    "- The reward is almost always zero. In this situation, the agent does not get positive or negative feedback. The majority of gradients become zero.\n",
    "- Only the actions we made to hit the ball were crucial to our victory, everything that happened after we struck the ball had no bearing on our victory.\n",
    "\n",
    "#### Solution\n",
    "- We set the reward of actions taken before each reward, similar to the reward obtained. For example if we got reward +1 at time 200, we say that reward of time 199 is +0.99, reward of time 198 is +0.98 and so on.\n",
    "- We have the rewards for actions that resulted in a +1 or -1 with this reward criteria. We presume that the closer the action is to the reward received, the more significant it is.\n",
    "- Some notes about normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy_network, env):\n",
    "    states = [] # shape = (x,80,80)\n",
    "    up_or_down_action=[]\n",
    "    rewards=[]\n",
    "    network_output=[]\n",
    "    observation = env.reset()\n",
    "    new_observation = observation\n",
    "    done = False\n",
    "    policy_output = []\n",
    "    while done == False:\n",
    "        processed_network_input = preprocess_frames(new_frame=new_observation,last_frame=observation)\n",
    "        states.append(processed_network_input)\n",
    "        reshaped_input = np.expand_dims(processed_network_input,axis=0) \n",
    "        up_probability = policy_network.predict(reshaped_input,batch_size=1)[0][0]\n",
    "        network_output.append(up_probability)\n",
    "        policy_output.append(up_probability)\n",
    "        actual_action = np.random.choice(a=[2,3],size=1,p=[up_probability,1-up_probability]) \n",
    "        # 2 is up and 3 is down \n",
    "        if actual_action==2:\n",
    "            up_or_down_action.append(1)\n",
    "        else:\n",
    "            up_or_down_action.append(0)\n",
    "        observation= new_observation\n",
    "        new_observation, reward, done, info = env.step(actual_action)\n",
    "        rewards.append(reward)  \n",
    "        if done:\n",
    "            break        \n",
    "    env.close()\n",
    "    return states, up_or_down_action,rewards, network_output\n",
    "def process_rewards(r):\n",
    "    reward_decay=0.99\n",
    "    tmp_r=0\n",
    "    rew=np.zeros_like(r,dtype=np.float16)\n",
    "    for i in range(len(r)-1,-1,-1):\n",
    "        if r[i]==0:\n",
    "            tmp_r=tmp_r*reward_decay\n",
    "            rew[i]=tmp_r\n",
    "        else: \n",
    "            tmp_r = r[i]\n",
    "            rew[i]=tmp_r\n",
    "    return rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56055f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#states, up_or_down_action, rewards, network_output = generate_episode(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7df62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of states= \"+str(len(states)))# this is the number of frames\n",
    "print(\"shape of each state=\"+str(states[0].shape))\n",
    "print(\"length of rewards= \"+str(len(rewards)))\n",
    "# lets see how many times we won through whole game:\n",
    "print(\"count win=\"+str(len(list(filter(lambda r: r>0,rewards)))))\n",
    "print(\"count lose=\"+str(len(list(filter(lambda r: r<0,rewards)))))\n",
    "print(\"count zero rewards=\"+str(len(list(filter(lambda r: r==0,rewards)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "up_or_down_action[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0f571",
   "metadata": {},
   "source": [
    "##### Because the network has never been trained, its output is always around 50%. indicating it doesn't know which option is best right now and gives all states a probability of approximately 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(process_rewards(rewards),'-',)\n",
    "ax=plt.gca()\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d0ad5",
   "metadata": {},
   "source": [
    "### Training and simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddddd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_batch(model, env, n_batches=10):\n",
    "    env = gym.make('Pong-v0')\n",
    "    batch = []\n",
    "    batch_up_or_down_action = []\n",
    "    batch_rewards = []\n",
    "    batch_network_output = []\n",
    "    for i in range(n_batches):\n",
    "        states,up_or_down_action,rewards,network_output = generate_episode(model, env)   \n",
    "        batch.extend(states[15:])\n",
    "        batch_network_output.extend(network_output[15:])\n",
    "        batch_up_or_down_action.extend(up_or_down_action[15:])\n",
    "        batch_rewards.extend(rewards[15:])\n",
    "    episode_reward = np.expand_dims(process_rewards(batch_rewards), 1)\n",
    "    X = np.array(batch)\n",
    "    y_tmp = np.array(batch_up_or_down_action)\n",
    "    y_true = np.expand_dims(y_tmp,1)\n",
    "    episode_reward = episode_reward.astype(np.int64)\n",
    "    print((episode_reward))\n",
    "    print((y_true))\n",
    "    policy_network_train.fit(x=[X, episode_reward], y=y_true)\n",
    "    return batch, batch_up_or_down_action, batch_rewards, batch_network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_n_times = 2000\n",
    "for i in range(train_n_times):\n",
    "    states, up_or_down_action, rewards, network_output = generate_episode_batch(model, env, 10)\n",
    "    if i%500==0:\n",
    "        print(\"i=\"+str(i))\n",
    "        rr = np.array(rewards)\n",
    "        print('count win='+str(len(rr[rr>0]))) \n",
    "        model.save(\"policy_network_model_simple.h5\")\n",
    "        model.save(\"policy_network_model_simple\"+str(i)+\".h5\")\n",
    "        with open('rews_model_simple.txt','a') as f_rew:\n",
    "            f_rew.write(\"i=\"+str(i)+'       reward= '+str(len(rr[rr > 0])))\n",
    "            f_rew.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
